\documentclass[11pt, letterpaper]{article}

\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{float}

% Modern font stack — Adobe Source family
\usepackage[default,regular]{sourceserifpro}   % Body text
\usepackage[semibold]{sourcesanspro}            % Sans-serif for headings
\usepackage[scale=0.88]{sourcecodepro}          % Monospace

% Colors
\definecolor{amdred}{HTML}{ED1C24}
\definecolor{novablue}{HTML}{1565C0}
\definecolor{darkgray}{HTML}{333333}
\definecolor{medgray}{HTML}{555555}

% Section headings — sans-serif, modern
\titlespacing*{\section}{0pt}{1.5ex plus 0.5ex}{0.8ex plus 0.2ex}
\titlespacing*{\subsection}{0pt}{1ex plus 0.3ex}{0.5ex plus 0.1ex}
\titleformat{\section}{\large\sffamily\bfseries\color{darkgray}}{\thesection}{0.8em}{}
\titleformat{\subsection}{\normalsize\sffamily\bfseries\color{medgray}}{\thesubsection}{0.8em}{}

% Compact floats
\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{2pt}

% Captions — sans-serif, slightly smaller
\captionsetup{font={small,sf}, labelfont={bf}}

% Header
\pagestyle{fancy}
\fancyhf{}
\rhead{\small\sffamily\color{gray}NOVA on MI300X --- Technical Report}
\lhead{\small\sffamily\color{gray}Jayant Lohia}
\rfoot{\small\sffamily\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\hypersetup{colorlinks=true, linkcolor=novablue, urlcolor=novablue, citecolor=novablue}

\begin{document}

% ═══════════════════════════════════════════════════════════════
% TITLE
% ═══════════════════════════════════════════════════════════════
\begin{center}
{\LARGE\sffamily\bfseries NOVA on MI300X: Production-Quality F(6,3)\\[4pt]
Winograd in FP16 That Beats MIOpen}\\[12pt]
{\large\sffamily Jayant Lohia}\\[4pt]
{\normalsize\sffamily\color{medgray} February 2026}\\[8pt]
\rule{0.6\textwidth}{0.5pt}
\end{center}

\vspace{0.5em}

% ═══════════════════════════════════════════════════════════════
% EXECUTIVE SUMMARY
% ═══════════════════════════════════════════════════════════════
\section{Executive Summary}

MIOpen has Winograd kernels only at F(2,3). There are no production kernels at larger tile sizes on any AMD GPU---and never have been. MIOpen's own codebase contains complete infrastructure for F(4,3) through F(6,3), but it was abandoned due to numerical instability in reduced precision.

\textbf{I built the missing kernel.} In two weeks, on a single MI300X, I implemented a full-performance F(6,3) Winograd convolution as a HIP kernel with PyTorch integration. The results:

\begin{itemize}[nosep, leftmargin=1.5em]
    \item \textbf{Beats MIOpen at batch=1 inference} by 17--57\% across all ResNet-50 layers.
    \item \textbf{Zero accuracy loss}: 63.29\% top-1 on ImageNetV2 (10K images) vs.\ 63.15\% FP32 baseline.
    \item \textbf{Zero NaN/Inf}: Standard F(6,3) produces 221,000 NaN values on the same test. NOVA produces zero.
    \item \textbf{Drop-in PyTorch replacement}: One function call replaces all eligible Conv2d layers in any model.
    \item \textbf{Stable Diffusion works}: 49/49 UNet convolutions replaced, valid 512$\times$512 images generated, at 0.98$\times$ MIOpen step latency.
\end{itemize}

The fix is mathematical, not architectural. NOVA selects optimal interpolation points for the Winograd transform that minimize condition numbers, reducing the maximum matrix entry from ${\sim}10$ to $2.72$---safely within FP16 dynamic range. This solves the exact instability that blocked AMD's prior attempts.

% ═══════════════════════════════════════════════════════════════
% WHAT I BUILT
% ═══════════════════════════════════════════════════════════════
\section{What I Built}

\subsection{HIP Kernel: Multi-Pass F(6,3) Winograd}

The kernel follows the same multi-pass architecture that MIOpen uses for F(2,3), extended to the larger F(6,3) tile size with NOVA's numerically stable transform matrices:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/architecture.png}
    \caption{NOVA F(6,3) multi-pass architecture. Input and output transforms are custom HIP kernels using wave shuffles (zero shared memory). The GEMM uses rocBLAS strided batched GEMM with FP32 accumulation via MFMA.}
    \label{fig:arch}
\end{figure}

\noindent Key implementation details:

\begin{itemize}[nosep, leftmargin=1.5em]
    \item \textbf{Transforms}: HIP kernels, 4 tiles per workgroup (256 threads = 4 wavefronts), register-only via \texttt{\_\_shfl} wave shuffles---zero LDS usage.
    \item \textbf{GEMM}: rocBLAS \texttt{gemm\_strided\_batched\_ex}---64 batches, FP16 inputs, FP32 accumulation via MFMA (\texttt{mfma\_f32\_16x16x16f16}).
    \item \textbf{Filter transform}: Computed once in FP32, cached as FP16. Zero cost on subsequent forward passes.
    \item \textbf{Precision}: FP16 throughout transforms (safe because NOVA's max entry is 2.72), FP32 accumulation in GEMM.
\end{itemize}

\subsection{PyTorch Integration}

The kernel is accessible from Python as a drop-in replacement for \texttt{torch.nn.Conv2d}:

\begin{verbatim}
    from nova_winograd_ext import replace_conv2d_with_nova

    model = torchvision.models.resnet50(pretrained=True).cuda().half()
    replace_conv2d_with_nova(model)  # That's it. 13 layers replaced.
    output = model(input)            # Uses NOVA F(6,3) automatically.
\end{verbatim}

\noindent Additional capabilities:
\begin{itemize}[nosep, leftmargin=1.5em]
    \item \texttt{NovaWinogradConv2d.from\_conv2d(conv)} --- single-layer drop-in replacement
    \item \texttt{NovaWinogradConv2dTrainable} --- HIP forward pass, FP32 native backward pass
    \item \texttt{NovaWinogradConv2dCompilable} --- compatible with \texttt{torch.compile(fullgraph=True)}
    \item Full \texttt{torch.autograd} support with verified gradients ($<$0.03\% error vs.\ native)
\end{itemize}

\noindent \textbf{Test suite}: 11/11 tests pass, covering correctness, accuracy, NaN safety, weight caching, backward pass, model surgery, training convergence, and \texttt{torch.compile}.

% ═══════════════════════════════════════════════════════════════
% RESULTS
% ═══════════════════════════════════════════════════════════════
\section{Results}

All results collected on AMD Instinct MI300X (304 CUs, 205.8\,GB HBM3, ROCm 6.3, PyTorch 2.9.1).

\subsection{Batch=1 Inference: NOVA Beats MIOpen}

At batch size 1---the latency-critical case for interactive inference and single-request serving---NOVA's F(6,3) beats MIOpen's native F(2,3) on \textbf{every ResNet-50 layer}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/latency_b1.png}
    \caption{Batch=1 latency comparison. NOVA F(6,3) is faster than MIOpen's production F(2,3) across all four ResNet-50 convolutional stages. The advantage grows with channel count (conv4\_x: 2.3$\times$ faster) because the 5.6$\times$ arithmetic reduction of F(6,3) vs.\ F(2,3) increasingly dominates.}
    \label{fig:latency}
\end{figure}

This is the headline result: \textbf{a kernel that AMD's own team couldn't ship now outperforms the one they did ship}, for the workload that matters most (latency-bound inference).

\subsection{Full Performance Profile}

\begin{table}[H]
\centering
\caption{Performance across batch sizes. NOVA wins at B=1 (latency-critical). MIOpen's fused F(2,3) kernel wins at larger batches due to single-dispatch advantage. The gap narrows with \texttt{fp16\_alt\_impl} (see Section~\ref{sec:future}).}
\label{tab:perf}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Config} & \textbf{MIOpen} & \textbf{NOVA HIP} & \textbf{Python Wino} & \textbf{HIP/MIO} & \textbf{HIP/Py} \\
\midrule
\rowcolor{green!8} conv2\_x [B=1] & 0.035\,ms & \textbf{0.029\,ms} & 0.427\,ms & 0.83$\times$ & 14.9$\times$ \\
\rowcolor{green!8} conv3\_x [B=1] & 0.036\,ms & \textbf{0.025\,ms} & 0.360\,ms & 0.71$\times$ & 14.3$\times$ \\
\rowcolor{green!8} conv4\_x [B=1] & 0.056\,ms & \textbf{0.024\,ms} & 0.332\,ms & 0.43$\times$ & 13.8$\times$ \\
\rowcolor{green!8} conv5\_x [B=1] & 0.050\,ms & \textbf{0.026\,ms} & 0.377\,ms & 0.51$\times$ & 14.7$\times$ \\
conv2\_x [B=8] & 0.036\,ms & 0.103\,ms & 1.484\,ms & 2.86$\times$ & 14.4$\times$ \\
conv3\_x [B=8] & 0.036\,ms & 0.074\,ms & 0.809\,ms & 2.07$\times$ & 11.0$\times$ \\
conv4\_x [B=8] & 0.051\,ms & 0.077\,ms & 0.663\,ms & 1.51$\times$ & 8.6$\times$ \\
conv5\_x [B=8] & 0.051\,ms & 0.087\,ms & 0.794\,ms & 1.70$\times$ & 9.1$\times$ \\
conv2\_x [B=32] & 0.085\,ms & 0.361\,ms & 5.593\,ms & 4.24$\times$ & 15.5$\times$ \\
conv3\_x [B=32] & 0.066\,ms & 0.223\,ms & 2.866\,ms & 3.36$\times$ & 12.9$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ImageNetV2: Zero Accuracy Loss}

Evaluated on ImageNetV2 matched-frequency (10,000 images, 1,000 classes) using pretrained ResNet-50 with 13 of 16 3$\times$3 convolutions replaced (3 stride-2 layers are ineligible for Winograd):

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/imagenet_accuracy.png}
    \caption{ImageNetV2 top-1 accuracy. NOVA F(6,3) in FP16 preserves full accuracy (63.29\%, McNemar $p=0.28$ vs.\ baseline---not statistically different). Standard F(6,3) in FP16 collapses to 31.07\% with 221,000 NaN values---a catastrophic 32-point accuracy drop.}
    \label{fig:accuracy}
\end{figure}

\begin{table}[H]
\centering
\caption{ImageNetV2 accuracy summary. NOVA F(6,3) in FP16 is the only large-tile configuration that preserves accuracy. Standard points are catastrophically broken.}
\label{tab:accuracy}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} & \textbf{NaN Count} & \textbf{$\Delta$ vs.\ FP32} \\
\midrule
FP32 Baseline (direct conv) & 63.15 & 84.58 & 0 & --- \\
MIOpen FP16 & 63.18 & 84.60 & 0 & +0.03\% \\
NOVA F(4,3) FP16 & 63.12 & 84.59 & 0 & $-$0.03\% \\
\textbf{NOVA F(6,3) FP16} & \textbf{63.29} & \textbf{84.60} & \textbf{0} & \textbf{+0.14\%} \\
Standard F(6,3) FP16 & \cellcolor{red!15}31.07 & \cellcolor{red!15}53.44 & \cellcolor{red!15}221,000 & \cellcolor{red!15}$-$32.08\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Numerical Stability}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/stability.png}
    \caption{Left: Relative error of FP16 convolution vs.\ FP32 direct convolution. NOVA achieves 5.2\% error; standard F(6,3) exceeds 200\%. Right: NaN/Inf counts---NOVA produces exactly zero across all tests, while standard points generate hundreds to hundreds of thousands.}
    \label{fig:stability}
\end{figure}

The stability improvement factor is \textbf{38.8$\times$} (relative error: 5.2\% NOVA vs.\ 201\% standard). This is the direct consequence of NOVA's condition number optimization: the F(6,3) $B^T$ matrix condition number drops from $\sim$100 (standard) to $\sim$8 (NOVA).

\subsection{Stable Diffusion: End-to-End Validation}

Stable Diffusion 1.5 is the ultimate stress test: the UNet runs 20+ denoising steps, each involving 49 eligible 3$\times$3 convolutions. Any numerical instability accumulates across steps, destroying the generated image.

\begin{table}[H]
\centering
\caption{Stable Diffusion 1.5 UNet benchmark. NOVA replaces all 49 eligible Conv2d layers and achieves parity with MIOpen at near-identical step latency, with zero numerical failures.}
\label{tab:sd}
\small
\begin{tabular}{lcc}
\toprule
& \textbf{MIOpen (Standard)} & \textbf{NOVA F(6,3)} \\
\midrule
Layers replaced & 0/49 & \textbf{49/49} \\
Step latency & 26.63\,ms & 27.15\,ms (0.98$\times$) \\
Rel.\ error vs.\ standard & --- & 3.97\% \\
NaN / Inf & 0 / 0 & \textbf{0 / 0} \\
Image quality & Baseline & Visually identical \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\textwidth]{figures/sd_output.png}
    \caption{Image generated by Stable Diffusion 1.5 with all 49 UNet convolutions replaced by NOVA F(6,3) in FP16. The image is coherent and artifact-free, confirming numerical stability across 20 denoising steps.}
    \label{fig:sd}
\end{figure}

% ═══════════════════════════════════════════════════════════════
% WHY THIS MATTERS
% ═══════════════════════════════════════════════════════════════
\section{Why This Matters for AMD}

\subsection{AMD Already Built the Infrastructure}

MIOpen's source code contains a complete multi-pass Winograd framework for F(4,3) through F(6,3): C++ solver classes, assembly transform templates, GEMM integration, and xDLOps (MFMA) variants. \textbf{None of it ever shipped.} There are zero performance database entries for these solvers on any GPU generation (gfx906, gfx908, gfx90a, gfx942). The infrastructure was abandoned because standard interpolation points are numerically unstable in FP16.

NOVA solves this exact problem. The kernel I built uses the same architectural pattern MIOpen's team designed---just with different transform matrices. \textbf{Integration into MIOpen would complete work that AMD already invested in.}

\subsection{Competitive Advantage Over NVIDIA}

NVIDIA has moved \textit{away} from Winograd:
\begin{itemize}[nosep, leftmargin=1.5em]
    \item cuDNN's maximum was F(4,3) in FP32 only---never F(6,3), never with Tensor Cores.
    \item Fused Winograd is explicitly blocked on Hopper (SM 90) and later.
    \item NVIDIA's strategy is to rely on raw Tensor Core throughput instead.
\end{itemize}

If AMD ships F(6,3) Winograd with NOVA points, it would offer a capability that \textbf{no NVIDIA GPU has}---a 5.6$\times$ arithmetic advantage over F(2,3) that NVIDIA cannot match with their current software stack.

\subsection{The \texttt{fp16\_alt} Opportunity}
\label{sec:future}

During rocBLAS tuning, I discovered that the \texttt{fp16\_alt\_impl} flag (an alternate FP16 MFMA datapath) provides \textbf{1.6--33$\times$ GEMM speedup} on rocBLAS 5 (ROCm 7.1). This flag is not available on the rocBLAS 4 bundled with PyTorch's current ROCm 6.3 wheels. When PyTorch ships ROCm 7.x support, the batch$>$1 performance gap closes significantly---\textbf{with zero code changes to the kernel}.

\subsection{Business Impact}

\begin{enumerate}[nosep, leftmargin=1.5em]
    \item \textbf{Inference latency}: Batch=1 wins directly translate to lower per-request latency for serving workloads.
    \item \textbf{Generative AI}: Stable Diffusion, Flux, and other diffusion models are 90\%+ convolutions. F(6,3) Winograd reduces arithmetic by 5.6$\times$ per layer.
    \item \textbf{Differentiation}: ``The only GPU platform with large-tile Winograd in FP16'' is a concrete, defensible marketing claim.
    \item \textbf{Low integration cost}: The architecture matches MIOpen's existing multi-pass framework. The delta is transform matrices and a new solver entry---not a rewrite.
\end{enumerate}

% ═══════════════════════════════════════════════════════════════
% REPRODUCTION
% ═══════════════════════════════════════════════════════════════
\section{Reproduction}

All code runs on a single MI300X with ROCm 6.3 and PyTorch 2.9.1. Three commands to reproduce:

\begin{verbatim}
    # Build
    hipcc -shared -fPIC -o libnova_winograd.so nova_winograd_v1.hip \
        -std=c++17 -lrocblas -lamdhip64 --offload-arch=gfx942

    # Test (11/11 pass)
    python test_nova_kernel.py

    # Benchmark
    python bench_nova_kernel.py
\end{verbatim}

\noindent \textbf{File inventory} (2,302 lines total):
\texttt{nova\_winograd\_v1.hip} (906 lines --- HIP kernels + C API),
\texttt{nova\_winograd\_ext.py} (537 --- PyTorch wrapper + model surgery),
\texttt{test\_nova\_kernel.py} (460 --- 11 correctness tests),
\texttt{nova\_sd\_demo.py} (234 --- SD benchmark),
\texttt{bench\_nova\_kernel.py} (165 --- performance benchmark).

% ═══════════════════════════════════════════════════════════════
% CONCLUSION
% ═══════════════════════════════════════════════════════════════
\section{Conclusion}

Large-tile Winograd convolution was abandoned by every major GPU vendor due to numerical instability in reduced precision. This report demonstrates that the instability is a \textit{point selection problem}, not a fundamental limitation. By using NOVA's optimized interpolation points, F(6,3) Winograd runs correctly in FP16 on AMD MI300X---beating MIOpen's own production F(2,3) kernel at inference latency, preserving full ImageNet accuracy, and generating valid Stable Diffusion images.

The kernel exists. It works. It's a drop-in. And it's faster.

\vspace{1em}

\noindent\rule{\textwidth}{0.4pt}
{\small\color{gray} Contact: Jayant Lohia. All experiments conducted on AMD Instinct MI300X VF, 304 CUs, 205.8\,GB HBM3. NOVA point selection from the NOVA paper (arXiv). Code available upon request.}

\end{document}
