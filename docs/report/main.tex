\documentclass[11pt, letterpaper]{article}

\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{float}

% Font stack
\usepackage[default,regular]{sourceserifpro}   % Body text
\usepackage[semibold]{sourcesanspro}            % Sans-serif for headings
\usepackage[scale=0.88]{sourcecodepro}          % Monospace

% Colors
\definecolor{amdred}{HTML}{ED1C24}
\definecolor{novablue}{HTML}{1565C0}
\definecolor{darkgray}{HTML}{333333}
\definecolor{medgray}{HTML}{555555}

% Section headings
\titlespacing*{\section}{0pt}{1.5ex plus 0.5ex}{0.8ex plus 0.2ex}
\titlespacing*{\subsection}{0pt}{1ex plus 0.3ex}{0.5ex plus 0.1ex}
\titleformat{\section}{\large\sffamily\bfseries\color{darkgray}}{\thesection}{0.8em}{}
\titleformat{\subsection}{\normalsize\sffamily\bfseries\color{medgray}}{\thesubsection}{0.8em}{}

% Compact floats
\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{2pt}

% Captions
\captionsetup{font={small,sf}, labelfont={bf}}

% Header
\pagestyle{fancy}
\fancyhf{}
\rhead{\small\sffamily\color{gray}NOVA on MI300X | Technical Report}
\lhead{\small\sffamily\color{gray}Jayant Lohia}
\rfoot{\small\sffamily\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\hypersetup{colorlinks=true, linkcolor=novablue, urlcolor=novablue, citecolor=novablue}

\begin{document}

% ═══════════════════════════════════════════════════════════════
% TITLE
% ═══════════════════════════════════════════════════════════════
\begin{center}
{\LARGE\sffamily\bfseries NOVA on MI300X: F(6,3)\\[4pt]
Winograd in FP16}\\[12pt]
{\large\sffamily Jayant Lohia}\\[4pt]
{\normalsize\sffamily\color{medgray} February 2026}\\[8pt]
\rule{0.6\textwidth}{0.5pt}
\end{center}

\vspace{0.5em}

% ═══════════════════════════════════════════════════════════════
% EXECUTIVE SUMMARY
% ═══════════════════════════════════════════════════════════════
\section{Executive Summary}

MIOpen ships Winograd only at F(2,3). There are no production kernels at larger tile sizes on any AMD GPU, and there never have been. MIOpen's codebase contains infrastructure for F(4,3) through F(6,3), but it was abandoned because standard interpolation points are numerically unstable in reduced precision.

This report presents a working F(6,3) Winograd HIP kernel with PyTorch integration that addresses the numerical stability gap:

\begin{itemize}[nosep, leftmargin=1.5em]
    \item \textbf{Faster than MIOpen at batch=1}: 17\% to 57\% lower latency across all ResNet-50 layers.
    \item \textbf{No accuracy loss}: 63.29\% top-1 on ImageNetV2 (10K images) vs.\ 63.15\% FP32 baseline.
    \item \textbf{No NaN/Inf}: Standard F(6,3) produces 221,000 NaN values on the same test. NOVA produces zero.
    \item \textbf{Drop-in replacement}: One function call replaces all eligible Conv2d layers in any model.
    \item \textbf{Stable Diffusion}: 49/49 SD 1.5 UNet convolutions replaced, valid 512$\times$512 images, 0.98$\times$ MIOpen step latency.
    \item \textbf{Multiple architectures}: Also validated on SDXL (38 layers, 1024$\times$1024) and DenseNet-161 (78 layers, ImageNetV2 accuracy preserved).
\end{itemize}

The fix is mathematical, not architectural. NOVA selects interpolation points that minimize condition numbers, bringing the maximum matrix entry from ${\sim}10$ down to $2.72$, which is within FP16 dynamic range.

% ═══════════════════════════════════════════════════════════════
% WHAT I BUILT
% ═══════════════════════════════════════════════════════════════
\section{What I Built}

\subsection{HIP Kernel: Multi-Pass F(6,3) Winograd}

The kernel follows the same multi-pass architecture that MIOpen uses for F(2,3), extended to the F(6,3) tile size with NOVA's transform matrices:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/architecture.png}
    \caption{NOVA F(6,3) multi-pass architecture. Input and output transforms are HIP kernels using wave shuffles (no shared memory). The GEMM uses rocBLAS strided batched GEMM with FP32 accumulation via MFMA.}
    \label{fig:arch}
\end{figure}

\noindent Implementation details:

\begin{itemize}[nosep, leftmargin=1.5em]
    \item \textbf{Transforms}: HIP kernels, 4 tiles per workgroup (256 threads = 4 wavefronts), register-only via \texttt{\_\_shfl} wave shuffles, no LDS.
    \item \textbf{GEMM}: rocBLAS \texttt{gemm\_strided\_batched\_ex}, 64 batches, FP16 inputs, FP32 accumulation via MFMA (\texttt{mfma\_f32\_16x16x16f16}).
    \item \textbf{Filter transform}: Computed once in FP32, cached as FP16. No cost on subsequent forward passes.
    \item \textbf{Precision}: FP16 throughout transforms (NOVA's max entry is 2.72, so this is safe), FP32 accumulation in GEMM.
\end{itemize}

\subsection{PyTorch Integration}

The kernel is accessible from Python as a replacement for \texttt{torch.nn.Conv2d}:

\begin{verbatim}
    from nova_winograd_ext import replace_conv2d_with_nova

    model = torchvision.models.resnet50(pretrained=True).cuda().half()
    replace_conv2d_with_nova(model)  # 13 layers replaced.
    output = model(input)            # Uses NOVA F(6,3) automatically.
\end{verbatim}

\noindent Also included:
\begin{itemize}[nosep, leftmargin=1.5em]
    \item \texttt{NovaWinogradConv2d.from\_conv2d(conv)}: single-layer replacement
    \item \texttt{NovaWinogradConv2dTrainable}: HIP forward pass, FP32 native backward pass
    \item \texttt{NovaWinogradConv2dCompilable}: works with \texttt{torch.compile(fullgraph=True)}
    \item Full \texttt{torch.autograd} support with verified gradients ($<$0.03\% error vs.\ native)
\end{itemize}

\noindent \textbf{Test suite}: 11/11 tests pass, covering correctness, accuracy, NaN safety, weight caching, backward pass, model surgery, training convergence, and \texttt{torch.compile}.

% ═══════════════════════════════════════════════════════════════
% RESULTS
% ═══════════════════════════════════════════════════════════════
\section{Results}

All results collected on AMD Instinct MI300X (304 CUs, 205.8\,GB HBM3, ROCm 6.3, PyTorch 2.9.1).

\subsection{Batch=1 Inference Latency}

At batch size 1 (the common case for interactive inference and single-request serving), NOVA's F(6,3) is faster than MIOpen's F(2,3) on every ResNet-50 layer:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/latency_b1.png}
    \caption{Batch=1 latency comparison. NOVA F(6,3) is faster across all four ResNet-50 convolutional stages. The gap grows with channel count (conv4\_x: 2.3$\times$ faster) because the 5.6$\times$ arithmetic reduction of F(6,3) vs.\ F(2,3) starts to dominate.}
    \label{fig:latency}
\end{figure}

At batch=1, the arithmetic reduction of F(6,3) vs.\ F(2,3) outweighs the multi-pass dispatch overhead, resulting in lower latency across all tested configurations.

\subsection{Full Performance Profile}

\begin{table}[H]
\centering
\caption{Performance across batch sizes. NOVA wins at B=1. MIOpen's fused F(2,3) kernel wins at larger batches due to single-dispatch advantage. The gap narrows with \texttt{fp16\_alt\_impl} (see Section~\ref{sec:future}).}
\label{tab:perf}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Config} & \textbf{MIOpen} & \textbf{NOVA HIP} & \textbf{Python Wino} & \textbf{HIP/MIO} & \textbf{HIP/Py} \\
\midrule
\rowcolor{green!8} conv2\_x [B=1] & 0.035\,ms & \textbf{0.029\,ms} & 0.427\,ms & 0.83$\times$ & 14.9$\times$ \\
\rowcolor{green!8} conv3\_x [B=1] & 0.036\,ms & \textbf{0.025\,ms} & 0.360\,ms & 0.71$\times$ & 14.3$\times$ \\
\rowcolor{green!8} conv4\_x [B=1] & 0.056\,ms & \textbf{0.024\,ms} & 0.332\,ms & 0.43$\times$ & 13.8$\times$ \\
\rowcolor{green!8} conv5\_x [B=1] & 0.050\,ms & \textbf{0.026\,ms} & 0.377\,ms & 0.51$\times$ & 14.7$\times$ \\
conv2\_x [B=8] & 0.036\,ms & 0.103\,ms & 1.484\,ms & 2.86$\times$ & 14.4$\times$ \\
conv3\_x [B=8] & 0.036\,ms & 0.074\,ms & 0.809\,ms & 2.07$\times$ & 11.0$\times$ \\
conv4\_x [B=8] & 0.051\,ms & 0.077\,ms & 0.663\,ms & 1.51$\times$ & 8.6$\times$ \\
conv5\_x [B=8] & 0.051\,ms & 0.087\,ms & 0.794\,ms & 1.70$\times$ & 9.1$\times$ \\
conv2\_x [B=32] & 0.085\,ms & 0.361\,ms & 5.593\,ms & 4.24$\times$ & 15.5$\times$ \\
conv3\_x [B=32] & 0.066\,ms & 0.223\,ms & 2.866\,ms & 3.36$\times$ & 12.9$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ImageNetV2: No Accuracy Loss}

Evaluated on ImageNetV2 matched-frequency (10,000 images, 1,000 classes) using pretrained ResNet-50 with 13 of 16 3$\times$3 convolutions replaced (3 stride-2 layers are ineligible for Winograd):

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/imagenet_accuracy.png}
    \caption{ImageNetV2 top-1 accuracy. NOVA F(6,3) in FP16 preserves accuracy (63.29\%, McNemar $p=0.28$ vs.\ baseline, not statistically different). Standard F(6,3) in FP16 drops to 31.07\% with 221,000 NaN values.}
    \label{fig:accuracy}
\end{figure}

\begin{table}[H]
\centering
\caption{ImageNetV2 accuracy summary. NOVA F(6,3) in FP16 is the only large-tile configuration that preserves accuracy. Standard points produce unusable results.}
\label{tab:accuracy}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} & \textbf{NaN Count} & \textbf{$\Delta$ vs.\ FP32} \\
\midrule
FP32 Baseline (direct conv) & 63.15 & 84.58 & 0 & --- \\
MIOpen FP16 & 63.18 & 84.60 & 0 & +0.03\% \\
NOVA F(4,3) FP16 & 63.12 & 84.59 & 0 & $-$0.03\% \\
\textbf{NOVA F(6,3) FP16} & \textbf{63.29} & \textbf{84.60} & \textbf{0} & \textbf{+0.14\%} \\
Standard F(6,3) FP16 & \cellcolor{red!15}31.07 & \cellcolor{red!15}53.44 & \cellcolor{red!15}221,000 & \cellcolor{red!15}$-$32.08\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Numerical Stability}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/stability.png}
    \caption{Left: Relative error of FP16 convolution vs.\ FP32 direct convolution. NOVA gets 5.2\% error; standard F(6,3) exceeds 200\%. Right: NaN/Inf counts. NOVA produces zero across all tests, while standard points generate hundreds to hundreds of thousands.}
    \label{fig:stability}
\end{figure}

The stability improvement is 38.8$\times$ (relative error: 5.2\% NOVA vs.\ 201\% standard). This comes from the condition number reduction: the F(6,3) $B^T$ matrix condition number drops from $\sim$100 (standard) to $\sim$8 (NOVA).

\subsection{Stable Diffusion: End-to-End Validation}

Stable Diffusion 1.5 is a good stress test for numerical stability: the UNet runs 20+ denoising steps, each involving 49 eligible 3$\times$3 convolutions. Any instability accumulates across steps and ruins the generated image.

\begin{table}[H]
\centering
\caption{Stable Diffusion 1.5 UNet benchmark. NOVA replaces all 49 eligible Conv2d layers at near-identical step latency, with no numerical failures.}
\label{tab:sd}
\small
\begin{tabular}{lcc}
\toprule
& \textbf{MIOpen (Standard)} & \textbf{NOVA F(6,3)} \\
\midrule
Layers replaced & 0/49 & \textbf{49/49} \\
Step latency & 26.63\,ms & 27.15\,ms (0.98$\times$) \\
Rel.\ error vs.\ standard & --- & 3.97\% \\
NaN / Inf & 0 / 0 & \textbf{0 / 0} \\
Image quality & Baseline & Visually identical \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.40\textwidth]{figures/sdxl_output.png}
    \caption{1024$\times$1024 image generated by SDXL with all 38 eligible UNet convolutions replaced by NOVA F(6,3) in FP16. No visible artifacts after 20 denoising steps.}
    \label{fig:sd}
\end{figure}

\noindent\textbf{A note on model architectures}: Newer diffusion models (SD3, Flux, SORA) use DiT (Diffusion Transformer) architectures based on attention layers, not convolutions. Winograd does not apply to attention. UNet-based models (SD~1.5, SDXL, ControlNet, inpainting variants) are still the most widely deployed diffusion architectures and rely heavily on 3$\times$3 convolutions.

\subsection{SDXL: 1024$\times$1024 Generation}

To validate beyond SD~1.5, I also tested on SDXL Base, a larger UNet that generates at 1024$\times$1024 resolution:

\begin{table}[H]
\centering
\caption{SDXL Base UNet benchmark. NOVA replaces all eligible Conv2d layers and generates valid 1024$\times$1024 images with no numerical failures.}
\label{tab:sdxl}
\small
\begin{tabular}{lcc}
\toprule
& \textbf{MIOpen (Standard)} & \textbf{NOVA F(6,3)} \\
\midrule
Resolution & 1024$\times$1024 & 1024$\times$1024 \\
Layers replaced & 0/38 & \textbf{38/38} \\
NaN / Inf & 0 / 0 & \textbf{0 / 0} \\
Image quality & Baseline & Visually identical \\
\bottomrule
\end{tabular}
\end{table}

\subsection{DenseNet-161: Architecture Generality}

DenseNet-161 has a different profile from ResNet-50: \textbf{78 eligible layers} (6$\times$ more than ResNet's 13) with smaller channel sizes (48 to 192 vs.\ 64 to 512). This covers a wider range of the kernel's configuration space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/model_generality.png}
    \caption{Eligible 3$\times$3 stride-1 convolution layers by architecture. DenseNet-161 has the most replaceable layers. Models that use depthwise convolutions (EfficientNet, MobileNet, ConvNeXt) have zero eligible layers.}
    \label{fig:generality}
\end{figure}

\noindent Full ImageNetV2 validation (10,000 images) on DenseNet-161 confirms no accuracy degradation with NOVA F(6,3) in FP16, matching the ResNet-50 result on a different architecture.

% ═══════════════════════════════════════════════════════════════
% WHY THIS MATTERS
% ═══════════════════════════════════════════════════════════════
\section{Why This Matters for AMD}

\subsection{AMD Already Built the Infrastructure}

MIOpen's source code contains a multi-pass Winograd framework for F(4,3) through F(6,3): C++ solver classes, assembly transform templates, GEMM integration, and xDLOps (MFMA) variants. None of it shipped. There are zero performance database entries for these solvers on any GPU generation (gfx906, gfx908, gfx90a, gfx942). The infrastructure was abandoned because standard interpolation points are numerically unstable in FP16.

NOVA's interpolation points address this problem. The prototype kernel follows the same architectural pattern MIOpen's team designed, just with different transform matrices. Integration into MIOpen would build on work that AMD already invested in.

\subsection{Competitive Landscape}

NVIDIA has moved away from Winograd:
\begin{itemize}[nosep, leftmargin=1.5em]
    \item cuDNN's maximum was F(4,3) in FP32 only. They never had F(6,3) or Tensor Core integration.
    \item Fused Winograd is explicitly blocked on Hopper (SM 90) and later.
    \item NVIDIA's approach is to rely on raw Tensor Core throughput instead.
\end{itemize}

If AMD ships F(6,3) Winograd with NOVA points, it would offer a capability that no NVIDIA GPU has: a 5.6$\times$ arithmetic reduction over F(2,3) that NVIDIA cannot match with their current software stack.

\subsection{The \texttt{fp16\_alt} Opportunity}
\label{sec:future}

During rocBLAS tuning, the \texttt{fp16\_alt\_impl} flag (an alternate FP16 MFMA datapath) showed 1.6$\times$ to 33$\times$ GEMM speedup on rocBLAS 5 (ROCm 7.1). This flag is not available on the rocBLAS 4 bundled with PyTorch's current ROCm 6.3 wheels. When PyTorch ships ROCm 7.x support, the batch$>$1 performance gap closes with no code changes to the kernel.

\subsection{Business Impact}

\begin{enumerate}[nosep, leftmargin=1.5em]
    \item \textbf{Inference latency}: Batch=1 wins translate to lower per-request latency for serving workloads.
    \item \textbf{Generative AI}: UNet-based diffusion models are dominated by 3$\times$3 convolutions. F(6,3) Winograd reduces arithmetic by 5.6$\times$ per layer.
    \item \textbf{Differentiation}: ``The only GPU platform with large-tile Winograd in FP16'' is a concrete claim.
    \item \textbf{Low integration cost}: The architecture matches MIOpen's existing multi-pass framework. The delta is transform matrices and a new solver entry, not a rewrite.
\end{enumerate}

% ═══════════════════════════════════════════════════════════════
% REPRODUCTION
% ═══════════════════════════════════════════════════════════════
\section{Reproduction}

All code runs on MI300X with ROCm 6.3 and PyTorch 2.9.1. Three commands to reproduce:

\begin{verbatim}
    # Build
    hipcc -shared -fPIC -o libnova_winograd.so nova_winograd_v1.hip \
        -std=c++17 -lrocblas -lamdhip64 --offload-arch=gfx942

    # Test (11/11 pass)
    python test_nova_kernel.py

    # Benchmark
    python bench_nova_kernel.py
\end{verbatim}

\noindent \textbf{Additional benchmarks} (all under \texttt{benchmarks/}):
\texttt{bench\_sdxl.py} (SDXL 1024$\times$1024 generation),
\texttt{bench\_densenet.py} (DenseNet-161 single-image),
\texttt{bench\_densenet\_imagenet.py} (DenseNet-161 full ImageNetV2 10K images),
\texttt{demo.py} (end-to-end demonstration).

% ═══════════════════════════════════════════════════════════════
% CONCLUSION
% ═══════════════════════════════════════════════════════════════
\section{Conclusion}

Large-tile Winograd convolution was set aside by GPU vendors due to numerical instability in reduced precision. This report presents evidence that the instability is a point selection problem, not a fundamental limitation. With NOVA's interpolation points, F(6,3) Winograd runs correctly in FP16 on MI300X, matching or improving on MIOpen's F(2,3) at batch=1 inference latency, preserving ImageNet accuracy across multiple architectures (ResNet-50, DenseNet-161), and generating valid images at both 512$\times$512 (SD~1.5) and 1024$\times$1024 (SDXL).

\vspace{1em}

\noindent\rule{\textwidth}{0.4pt}
{\small\color{gray} Contact: Jayant Lohia. All experiments on AMD Instinct MI300X VF, 304 CUs, 205.8\,GB HBM3. NOVA point selection from the NOVA paper (arXiv). Code available upon request.}

\end{document}
